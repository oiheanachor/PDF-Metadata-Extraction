{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e98bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "REV Extractor — Proximity & Layout Aware (v2)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse, logging, re, math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "# import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import fitz\n",
    "# from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "LOG = logging.getLogger(\"rev_extractor_proximity_v2\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "REV_VALUE_RE = re.compile(r\"^(?:[A-Z]{1,2}|\\d{1,2}-\\d{1,2})$\")\n",
    "REV_TOKEN_RE = re.compile(r\"^rev\\.?$\", re.IGNORECASE)\n",
    "TITLE_ANCHORS = {\"DWG\", \"DWG.\", \"DWGNO\", \"SHEET\", \"SCALE\", \"WEIGHT\", \"SIZE\", \"TITLE\"}\n",
    "REV_TABLE_HEADERS = {\"REVISIONS\", \"DESCRIPTION\", \"EC\", \"DFT\", \"APPR\", \"APPD\", \"DATE\"}\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    text: str\n",
    "    conf: Optional[float]\n",
    "    x: float\n",
    "    y: float\n",
    "    w: float\n",
    "    h: float\n",
    "\n",
    "@dataclass\n",
    "class PageResult:\n",
    "    tokens: List[Token]\n",
    "    text: str\n",
    "    engine: str\n",
    "\n",
    "@dataclass\n",
    "class RevHit:\n",
    "    file: str\n",
    "    page: int\n",
    "    value: str\n",
    "    engine: str\n",
    "    score: float\n",
    "    context_snippet: str\n",
    "\n",
    "def _scalarize(v):\n",
    "    \"\"\"Coerce any non-scalar to a plain Python scalar or string.\"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(v, np.ndarray):\n",
    "            return \", \".join(map(str, v.flatten().tolist()))\n",
    "        if isinstance(v, np.generic):\n",
    "            try:\n",
    "                return v.item()\n",
    "            except Exception:\n",
    "                return str(v)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(v, (list, tuple, set)):\n",
    "        return \", \".join(map(str, v))\n",
    "    if isinstance(v, dict):\n",
    "        return \", \".join(f\"{k}={str(vv)}\" for k,vv in v.items())\n",
    "    if isinstance(v, (bytes, bytearray)):\n",
    "        return v.decode(\"utf-8\", errors=\"ignore\")\n",
    "    # convert numpy scalar types if present\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(v, (np.integer, np.floating, np.bool_)):\n",
    "            return v.item()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(v, (str, int, float, bool)):\n",
    "        return v\n",
    "    return str(v)\n",
    "\n",
    "def norm_val(v: Any) -> str:\n",
    "    \"\"\"Normalize token text for comparisons: coerce to str, collapse whitespace, strip.\"\"\"\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    s = str(v)\n",
    "    # replace non-breaking spaces and collapse runs of whitespace to single space\n",
    "    s = s.replace(\"\\u00A0\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def in_bottom_right(x: float, y: float, width: float, height: float) -> bool:\n",
    "    return x > width * 0.55 and y > height * 0.60\n",
    "\n",
    "def distance(a: Tuple[float, float], b: Tuple[float, float]) -> float:\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def context_snippet_from_tokens(tokens: List[Token], center: Tuple[float, float], radius: float = 160) -> str:\n",
    "    close = [t.text for t in tokens if distance((t.x, t.y), center) <= radius]\n",
    "    import re as _re\n",
    "    s = \" \".join(close)\n",
    "    s = _re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s[:50]\n",
    "\n",
    "def get_native_tokens(pdf_path: Path, page_index0: int) -> PageResult:\n",
    "    tokens: List[Token] = []\n",
    "    text_parts: List[str] = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        page = doc[page_index0]\n",
    "        for x0, y0, x1, y1, txt, *_ in page.get_text(\"words\"):\n",
    "            txt_clean = txt.strip()\n",
    "            if not txt_clean:\n",
    "                continue\n",
    "            cx = (x0 + x1) / 2.0\n",
    "            cy = (y0 + y1) / 2.0\n",
    "            tokens.append(Token(text=txt_clean, conf=None, x=cx, y=cy, w=(x1-x0), h=(y1-y0)))\n",
    "            text_parts.append(txt_clean)\n",
    "    return PageResult(tokens=tokens, text=\" \".join(text_parts), engine=\"native\")\n",
    "\n",
    "class PaddleWrapper:\n",
    "    def __init__(self):\n",
    "        from paddleocr import PaddleOCR\n",
    "        self.ocr = PaddleOCR(lang=\"en\", use_angle_cls=True, show_log=False)\n",
    "\n",
    "    def run(self, image_bgr):\n",
    "        result = self.ocr.ocr(image_bgr, cls=True)\n",
    "        tokens: List[Token] = []\n",
    "        lines: List[str] = []\n",
    "        for det in result:\n",
    "            for (box, (txt, cf)) in det:\n",
    "                txt_clean = txt.strip()\n",
    "                if not txt_clean:\n",
    "                    continue\n",
    "                xs = [p[0] for p in box]; ys = [p[1] for p in box]\n",
    "                cx, cy = sum(xs)/4.0, sum(ys)/4.0\n",
    "                w = (max(xs)-min(xs)) or 1.0; h = (max(ys)-min(ys)) or 1.0\n",
    "                tokens.append(Token(text=txt_clean, conf=float(cf), x=cx, y=cy, w=w, h=h))\n",
    "                lines.append(txt_clean)\n",
    "        return PageResult(tokens=tokens, text=\" \".join(lines), engine=\"paddleocr\")\n",
    "\n",
    "class EasyWrapper:\n",
    "    def __init__(self):\n",
    "        import easyocr\n",
    "        self.reader = easyocr.Reader([\"en\"], gpu=False)\n",
    "\n",
    "    def run(self, image_bgr):\n",
    "        result = self.reader.readtext(image_bgr)\n",
    "        tokens: List[Token] = []\n",
    "        lines: List[str] = []\n",
    "        for (box, txt, cf) in result:\n",
    "            txt_clean = txt.strip()\n",
    "            if not txt_clean:\n",
    "                continue\n",
    "            xs = [p[0] for p in box]; ys = [p[1] for p in box]\n",
    "            cx, cy = sum(xs)/4.0, sum(ys)/4.0\n",
    "            w = (max(xs)-min(xs)) or 1.0; h = (max(ys)-min(ys)) or 1.0\n",
    "            tokens.append(Token(text=txt_clean, conf=float(cf), x=cx, y=cy, w=w, h=h))\n",
    "            lines.append(txt_clean)\n",
    "        return PageResult(tokens=tokens, text=\" \".join(lines), engine=\"easyocr\")\n",
    "\n",
    "def score_candidates(tokens: List[Token], page_w: float, page_h: float):\n",
    "    anchor_tokens = [t for t in tokens if norm_val(t.text).upper() in TITLE_ANCHORS]\n",
    "    rev_tokens = [t for t in tokens if REV_TOKEN_RE.match(norm_val(t.text))]\n",
    "    if not rev_tokens:\n",
    "        return None\n",
    "\n",
    "    def nearby_anchor_bonus(center_xy, radius=220):\n",
    "        return sum(1 for a in anchor_tokens if distance((a.x,a.y), center_xy) <= radius)\n",
    "\n",
    "    cands = []\n",
    "    for r in rev_tokens:\n",
    "        r_word = norm_val(r.text).lower()\n",
    "        is_revision_word = r_word.startswith(\"revision\")\n",
    "        neighborhood = [t for t in tokens if distance((t.x,t.y),(r.x,r.y)) <= 280]\n",
    "        looks_like_revision_table = any(norm_val(n.text).upper() in REV_TABLE_HEADERS for n in neighborhood)\n",
    "\n",
    "        for t in neighborhood:\n",
    "            v = norm_val(t.text)\n",
    "            if not REV_VALUE_RE.match(v):\n",
    "                continue\n",
    "            d = distance((t.x,t.y),(r.x,r.y)) + 1e-3\n",
    "            same_line = abs(t.y - r.y) <= max(r.h, t.h) * 0.8\n",
    "            to_right = t.x > r.x\n",
    "            base = 1000.0/d\n",
    "            if same_line: base += 4.0\n",
    "            if to_right: base += 6.0\n",
    "            if in_bottom_right(t.x,t.y,page_w,page_h): base += 5.0\n",
    "            base += nearby_anchor_bonus((t.x,t.y))*1.5\n",
    "            if t.conf is not None: base += (t.conf - 0.5)*2.0\n",
    "            if is_revision_word: base -= 2.0\n",
    "            if looks_like_revision_table: base -= 6.0\n",
    "\n",
    "            cands.append((base, v, (t.x,t.y)))\n",
    "\n",
    "    if not cands:\n",
    "        return None\n",
    "\n",
    "    br_cands = [c for c in cands if in_bottom_right(c[2][0], c[2][1], page_w, page_h)]\n",
    "    pool = br_cands if br_cands else cands\n",
    "\n",
    "    best = max(pool, key=lambda c: c[0])\n",
    "    score, v, center = best\n",
    "    ctx = context_snippet_from_tokens(tokens, center, radius=160)\n",
    "    return (v, score, center, ctx)\n",
    "\n",
    "def rasterize_to_bgr(pdf_path: Path, page_index0: int, dpi: int):\n",
    "    import numpy as np, cv2, fitz\n",
    "    try:\n",
    "        from PIL import Image\n",
    "    except Exception:\n",
    "        Image = None\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        page = doc[page_index0]\n",
    "        zoom = dpi / 72.0\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        # try to request RGB directly (PyMuPDF may accept colorspace)\n",
    "        try:\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False, colorspace=fitz.csRGB)\n",
    "        except TypeError:\n",
    "            # older versions may not accept colorspace kw; try default\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "\n",
    "        # fast path: raw samples -> numpy\n",
    "        buf = getattr(pix, \"samples\", None)\n",
    "        ncomps = getattr(pix, \"n\", None)\n",
    "        try:\n",
    "            if buf and ncomps:\n",
    "                arr = np.frombuffer(buf, dtype=np.uint8)\n",
    "                # validate expected size for common cases\n",
    "                if ncomps == 3 and arr.size == int(pix.w) * int(pix.h) * 3:\n",
    "                    img_rgb = arr.reshape((pix.h, pix.w, 3))\n",
    "                    img_rgb = np.ascontiguousarray(img_rgb)\n",
    "                    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "                if ncomps == 1 and arr.size == int(pix.w) * int(pix.h):\n",
    "                    img_gray = arr.reshape((pix.h, pix.w))\n",
    "                    img_gray = np.ascontiguousarray(img_gray)\n",
    "                    img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "                if ncomps == 4 and arr.size == int(pix.w) * int(pix.h) * 4:\n",
    "                    img_rgba = arr.reshape((pix.h, pix.w, 4))\n",
    "                    img_rgba = np.ascontiguousarray(img_rgba)\n",
    "                    img_bgr = cv2.cvtColor(img_rgba, cv2.COLOR_RGBA2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            # fall through to robust fallback\n",
    "            pass\n",
    "\n",
    "        # fallback A: ask for PNG bytes then decode (robust across colorspaces)\n",
    "        try:\n",
    "            png = None\n",
    "            if hasattr(pix, \"tobytes\"):\n",
    "                try:\n",
    "                    png = pix.tobytes(\"png\")\n",
    "                except Exception:\n",
    "                    png = None\n",
    "            if not png and hasattr(pix, \"getPNGData\"):\n",
    "                try:\n",
    "                    png = pix.getPNGData()\n",
    "                except Exception:\n",
    "                    png = None\n",
    "            if png:\n",
    "                arr = np.frombuffer(png, dtype=np.uint8)\n",
    "                img = cv2.imdecode(arr, cv2.IMREAD_COLOR)  # returns BGR or None\n",
    "                if isinstance(img, np.ndarray):\n",
    "                    return np.ascontiguousarray(img), float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # fallback B: use PIL if available\n",
    "        try:\n",
    "            if Image is not None:\n",
    "                mode = None\n",
    "                if ncomps == 1:\n",
    "                    mode = \"L\"\n",
    "                elif ncomps == 3:\n",
    "                    mode = \"RGB\"\n",
    "                elif ncomps == 4:\n",
    "                    mode = \"RGBA\"\n",
    "                else:\n",
    "                    mode = \"RGB\"\n",
    "                pil = Image.frombytes(mode, (pix.w, pix.h), pix.samples)\n",
    "                arr = np.asarray(pil)\n",
    "                if arr.ndim == 2:\n",
    "                    img_bgr = cv2.cvtColor(arr, cv2.COLOR_GRAY2BGR)\n",
    "                else:\n",
    "                    if arr.shape[2] == 4:\n",
    "                        img_bgr = cv2.cvtColor(arr, cv2.COLOR_RGBA2BGR)\n",
    "                    else:\n",
    "                        img_bgr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n",
    "                return np.ascontiguousarray(img_bgr), float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        raise ValueError(f\"Unable to rasterize {pdf_path.name} p{page_index0+1} to a valid BGR ndarray\")\n",
    "\n",
    "def analyze_page(pdf_path: Path, page_index0: int, dpi: int, use_paddle: bool, use_easy: bool):\n",
    "    native = get_native_tokens(pdf_path, page_index0)\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        pw, ph = doc[page_index0].rect.width, doc[page_index0].rect.height\n",
    "    if native.tokens:\n",
    "        res = score_candidates(native.tokens, pw, ph)\n",
    "        if res:\n",
    "            v, score, _, ctx = res\n",
    "            return (\"native\", v, score, ctx)\n",
    "    if native.text:\n",
    "        m = re.search(r\"(?i)\\brev(?:ision)?\\b\\s*[:#\\-]?\\s*([A-Za-z]{1,2}|\\d{1,2}-\\d{1,2})\\b\", native.text)\n",
    "        if m:\n",
    "            return (\"native_text\", norm_val(m.group(1)), 0.3, native.text[:50])\n",
    "\n",
    "    # rasterize -> OCR fallbacks; guard rasterization errors\n",
    "    try:\n",
    "        image_bgr, iw, ih = rasterize_to_bgr(pdf_path, page_index0, dpi)\n",
    "    except Exception as e:\n",
    "        LOG.warning(f\"Rasterization failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # validate image before OCR\n",
    "    import numpy as _np\n",
    "    if not (isinstance(image_bgr, _np.ndarray) and image_bgr.ndim == 3 and image_bgr.shape[2] == 3 and image_bgr.dtype == _np.uint8):\n",
    "        LOG.warning(f\"Rasterized image invalid for OCR p{page_index0+1} {pdf_path.name}: shape={getattr(image_bgr,'shape',None)} dtype={getattr(image_bgr,'dtype',None)}\")\n",
    "        return None\n",
    "\n",
    "    if use_paddle:\n",
    "        try:\n",
    "            padd = PaddleWrapper().run(image_bgr)\n",
    "            res = score_candidates(padd.tokens, iw, ih)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"paddleocr\", v, score, ctx)\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"PaddleOCR failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "    if use_easy:\n",
    "        try:\n",
    "            easy = EasyWrapper().run(image_bgr)\n",
    "            res = score_candidates(easy.tokens, iw, ih)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"easyocr\", v, score, ctx)\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"EasyOCR failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_pdf(pdf_path: Path, dpi: int, use_paddle: bool, use_easy: bool) -> Dict[int, RevHit]:\n",
    "    hits: Dict[int, RevHit] = {}\n",
    "    with fitz.open(pdf_path) as d:\n",
    "        n = len(d)\n",
    "    for i in range(n):\n",
    "        res = analyze_page(pdf_path, i, dpi, use_paddle, use_easy)\n",
    "        if not res: \n",
    "            continue\n",
    "        engine, value, score, ctx = res\n",
    "        if (i+1) not in hits or score > hits[i+1].score:\n",
    "            hits[i+1] = RevHit(file=pdf_path.name, page=i+1, value=value, engine=engine, score=score, context_snippet=ctx)\n",
    "    return hits\n",
    "\n",
    "def iter_pdfs(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield each PDF in folder once (case-insensitive).\"\"\"\n",
    "    # use iterdir + suffix.lower() to avoid duplicate matches on Windows\n",
    "    for p in folder.iterdir():\n",
    "        try:\n",
    "            if p.is_file() and p.suffix.lower() == \".pdf\":\n",
    "                yield p\n",
    "        except Exception:\n",
    "            # skip weird filesystem entries\n",
    "            continue\n",
    "\n",
    "def aggregate_rows(file_hits: Dict[int, RevHit]) -> List[Dict[str,Any]]:\n",
    "    # Return a single row per file containing only 'file' and 'value'.\n",
    "    # Choose the best-scoring hit for the file (if any).\n",
    "    if not file_hits:\n",
    "        return []\n",
    "    best = max(file_hits.values(), key=lambda h: getattr(h, 'score', 0))\n",
    "    return [{\n",
    "        \"file\": best.file,\n",
    "        \"value\": norm_val(best.value)\n",
    "    }]\n",
    "\n",
    "def run_pipeline(input_folder: Path, output_csv: Path, dpi: int, enable_paddle: bool, enable_easy: bool) -> List[Dict[str,Any]]:\n",
    "    all_rows: List[Dict[str,Any]] = []\n",
    "    pdfs = list(iter_pdfs(input_folder))\n",
    "    # defensive dedupe by resolved path (preserves order from iterdir)\n",
    "    seen = set()\n",
    "    unique_pdfs: List[Path] = []\n",
    "    for p in pdfs:\n",
    "        try:\n",
    "            rp = p.resolve()\n",
    "        except Exception:\n",
    "            rp = p\n",
    "        if rp in seen:\n",
    "            continue\n",
    "        seen.add(rp)\n",
    "        unique_pdfs.append(p)\n",
    "    pdfs = unique_pdfs\n",
    "    if not pdfs:\n",
    "        LOG.warning(f\"No PDFs found in {input_folder}\")\n",
    "    for p in tqdm(pdfs, desc=\"Scanning PDFs\"):\n",
    "        try:\n",
    "            file_hits = process_pdf(p, dpi, enable_paddle, enable_easy)\n",
    "            rows = aggregate_rows(file_hits)\n",
    "            if not rows:\n",
    "                # ensure one row per file even with no hits\n",
    "                all_rows.append({\"file\": p.name, \"value\": \"\"})\n",
    "            else:\n",
    "                all_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"Failed {p.name}: {e}\")\n",
    "            all_rows.append({\"file\": p.name, \"value\": \"\"})\n",
    "    # --- sanitize all rows BEFORE writing CSV ---\n",
    "    safe_rows = []\n",
    "    for i, row in enumerate(all_rows):\n",
    "        if not isinstance(row, dict):\n",
    "            LOG.warning(f\"Row {i} is not a dict: {row}\")\n",
    "            row = {\"file\": \"\", \"value\": str(row)}\n",
    "        sanitized: Dict[str, Any] = {}\n",
    "        for k in (\"file\", \"value\"):\n",
    "            try:\n",
    "                sanitized[k] = _scalarize(row.get(k, \"\"))\n",
    "            except Exception:\n",
    "                sanitized[k] = str(row.get(k, \"\"))\n",
    "        safe_rows.append(sanitized)\n",
    "    all_rows = safe_rows\n",
    "\n",
    "    # write CSV directly from sanitized rows (avoid pandas list-of-dicts issues)\n",
    "    try:\n",
    "        import csv\n",
    "        output_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "        seen_files = set()\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as outf:\n",
    "            writer = csv.writer(outf)\n",
    "            writer.writerow(['file', 'value'])\n",
    "            for r in all_rows:\n",
    "                f = r.get('file', '')\n",
    "                v = r.get('value', '')\n",
    "                try: fs = str(f)\n",
    "                except Exception: fs = repr(f)\n",
    "                try: vs = str(v)\n",
    "                except Exception: vs = repr(v)\n",
    "                if fs in seen_files:\n",
    "                    continue\n",
    "                seen_files.add(fs)\n",
    "                writer.writerow([fs, vs])\n",
    "        LOG.info(f\"Wrote CSV directly to {output_csv.resolve()} with {len(seen_files)} rows\")\n",
    "    except Exception as e:\n",
    "        LOG.error(f\"Failed to write CSV directly: {e}\")\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "def parse_args(argv=None):\n",
    "    a = argparse.ArgumentParser(description=\"Extract REV values using proximity & OCR fallbacks (v2).\")\n",
    "    a.add_argument(\"input_folder\", type=Path)\n",
    "    a.add_argument(\"-o\",\"--output\", type=Path, default=Path(\"rev_results.csv\"))\n",
    "    a.add_argument(\"--dpi\", type=int, default=240)\n",
    "    a.add_argument(\"--no-paddle\", action=\"store_true\")\n",
    "    a.add_argument(\"--no-easy\", action=\"store_true\")\n",
    "    return a.parse_args(argv)\n",
    "\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "    return run_pipeline(args.input_folder, args.output, args.dpi, enable_paddle=not args.no_paddle, enable_easy=not args.no_easy)\n",
    "\n",
    "def _in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\" and not _in_notebook():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "REV Extractor — Bottom-Right-First (Native → OCR fallback)\n",
    "Outputs exactly one row per input PDF: file, value, engine\n",
    "- Native vector text pass first; only files with no native hit fall back to OCR\n",
    "- Strict bottom-right ROI prioritization (title block)\n",
    "- 'OF' inside ROI mapped to 'EMPTY'\n",
    "- Fix for 'L' beating '1-0': edge exclusion + neighborhood assembly + pattern-aware scoring\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse, logging, re, math, csv\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "LOG = logging.getLogger(\"rev_extractor_br_first\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "# ----------------------------- Patterns & Constants -----------------------------\n",
    "\n",
    "# Allowed REV value patterns:\n",
    "# - 1–2 letters (A..Z or AA)\n",
    "# - hyphenated numeric pair (e.g., 1-0, 12-01)\n",
    "REV_VALUE_RE = re.compile(r\"^(?:[A-Z]{1,2}|\\d{1,2}-\\d{1,2})$\")\n",
    "\n",
    "# Tokens to detect a REV label\n",
    "REV_TOKEN_RE = re.compile(r\"^rev\\.?$\", re.IGNORECASE)\n",
    "\n",
    "# Weak anchors often present near title blocks\n",
    "TITLE_ANCHORS = {\"DWG\", \"DWG.\", \"DWGNO\", \"SHEET\", \"SCALE\", \"WEIGHT\", \"SIZE\", \"TITLE\"}\n",
    "\n",
    "# Headers around revision tables (down-weighted in global fallback)\n",
    "REV_TABLE_HEADERS = {\"REVISIONS\", \"DESCRIPTION\", \"EC\", \"DFT\", \"APPR\", \"APPD\", \"DATE\", \"CHKD\", \"DRAWN\"}\n",
    "\n",
    "# ROI defaults for bottom-right title block (can be adjusted via CLI)\n",
    "DEFAULT_BR_X = 0.68\n",
    "DEFAULT_BR_Y = 0.72\n",
    "\n",
    "# Edge margin to exclude tokens too close to page borders (grid letters/numbers)\n",
    "DEFAULT_EDGE_MARGIN = 0.018  # ~1.8% of page width/height (tune 0.015–0.025)\n",
    "\n",
    "# Two-letter junk frequently seen in title blocks; keep 'OF' allowed (used to infer EMPTY)\n",
    "DEFAULT_REV_2L_BLOCKLIST = {\"EC\", \"DF\", \"DT\", \"AP\", \"ID\", \"NO\", \"IN\", \"ON\", \"BY\"}\n",
    "\n",
    "# ----------------------------- Data Structures ---------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    text: str\n",
    "    conf: Optional[float]\n",
    "    x: float\n",
    "    y: float\n",
    "    w: float\n",
    "    h: float\n",
    "\n",
    "@dataclass\n",
    "class PageResult:\n",
    "    tokens: List[Token]\n",
    "    text: str\n",
    "    engine: str\n",
    "\n",
    "@dataclass\n",
    "class RevHit:\n",
    "    file: str\n",
    "    page: int\n",
    "    value: str\n",
    "    engine: str\n",
    "    score: float\n",
    "    context_snippet: str\n",
    "\n",
    "# ----------------------------- Utilities ---------------------------------------\n",
    "\n",
    "def _scalarize(v: Any):\n",
    "    \"\"\"Coerce any non-scalar to a plain Python scalar or string.\"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(v, np.ndarray):\n",
    "            return \", \".join(map(str, v.flatten().tolist()))\n",
    "        if isinstance(v, np.generic):\n",
    "            try:\n",
    "                return v.item()\n",
    "            except Exception:\n",
    "                return str(v)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(v, (list, tuple, set)):\n",
    "        return \", \".join(map(str, v))\n",
    "    if isinstance(v, dict):\n",
    "        return \", \".join(f\"{k}={str(vv)}\" for k, vv in v.items())\n",
    "    if isinstance(v, (bytes, bytearray)):\n",
    "        return v.decode(\"utf-8\", errors=\"ignore\")\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(v, (np.integer, np.floating, np.bool_)):\n",
    "            return v.item()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(v, (str, int, float, bool)):\n",
    "        return v\n",
    "    return str(v)\n",
    "\n",
    "def norm_val(v: Any) -> str:\n",
    "    \"\"\"Normalize token text for comparisons.\"\"\"\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    s = str(v).replace(\"\\u00A0\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def in_bottom_right(x: float, y: float, width: float, height: float) -> bool:\n",
    "    return x > width * 0.55 and y > height * 0.60\n",
    "\n",
    "def in_bottom_right_strict(x: float, y: float, width: float, height: float, brx: float, bry: float) -> bool:\n",
    "    return x >= width * brx and y >= height * bry\n",
    "\n",
    "def is_far_from_edges(x: float, y: float, width: float, height: float, edge_margin: float) -> bool:\n",
    "    \"\"\"Filter out tokens too close to page edges (removes border grid labels like K/L/16).\"\"\"\n",
    "    xm = width * edge_margin\n",
    "    ym = height * edge_margin\n",
    "    return (x > xm) and (x < width - xm) and (y > ym) and (y < height - ym)\n",
    "\n",
    "def distance(a: Tuple[float, float], b: Tuple[float, float]) -> float:\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def context_snippet_from_tokens(tokens: List[Token], center: Tuple[float, float], radius: float = 160) -> str:\n",
    "    close = [t.text for t in tokens if distance((t.x, t.y), center) <= radius]\n",
    "    s = \" \".join(close)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s[:80]\n",
    "\n",
    "# ----------------------------- Native Tokenization ------------------------------\n",
    "\n",
    "def get_native_tokens(pdf_path: Path, page_index0: int) -> PageResult:\n",
    "    tokens: List[Token] = []\n",
    "    text_parts: List[str] = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        page = doc[page_index0]\n",
    "        for x0, y0, x1, y1, txt, *_ in page.get_text(\"words\"):\n",
    "            txt_clean = txt.strip()\n",
    "            if not txt_clean:\n",
    "                continue\n",
    "            cx = (x0 + x1) / 2.0\n",
    "            cy = (y0 + y1) / 2.0\n",
    "            tokens.append(Token(text=txt_clean, conf=None, x=cx, y=cy, w=(x1-x0), h=(y1-y0)))\n",
    "            text_parts.append(txt_clean)\n",
    "    return PageResult(tokens=tokens, text=\" \".join(text_parts), engine=\"native\")\n",
    "\n",
    "# ----------------------------- OCR Wrappers ------------------------------------\n",
    "\n",
    "class PaddleWrapper:\n",
    "    def __init__(self):\n",
    "        from paddleocr import PaddleOCR\n",
    "        self.ocr = PaddleOCR(lang=\"en\", use_angle_cls=True, show_log=False)\n",
    "\n",
    "    def run(self, image_bgr):\n",
    "        result = self.ocr.ocr(image_bgr, cls=True)\n",
    "        tokens: List[Token] = []\n",
    "        lines: List[str] = []\n",
    "        for det in result:\n",
    "            for (box, (txt, cf)) in det:\n",
    "                txt_clean = txt.strip()\n",
    "                if not txt_clean:\n",
    "                    continue\n",
    "                xs = [p[0] for p in box]; ys = [p[1] for p in box]\n",
    "                cx, cy = sum(xs)/4.0, sum(ys)/4.0\n",
    "                w = (max(xs)-min(xs)) or 1.0; h = (max(ys)-min(ys)) or 1.0\n",
    "                tokens.append(Token(text=txt_clean, conf=float(cf), x=cx, y=cy, w=w, h=h))\n",
    "                lines.append(txt_clean)\n",
    "        return PageResult(tokens=tokens, text=\" \".join(lines), engine=\"paddleocr\")\n",
    "\n",
    "class EasyWrapper:\n",
    "    def __init__(self):\n",
    "        import easyocr\n",
    "        self.reader = easyocr.Reader([\"en\"], gpu=False)\n",
    "\n",
    "    def run(self, image_bgr):\n",
    "        result = self.reader.readtext(image_bgr)\n",
    "        tokens: List[Token] = []\n",
    "        lines: List[str] = []\n",
    "        for (box, txt, cf) in result:\n",
    "            txt_clean = txt.strip()\n",
    "            if not txt_clean:\n",
    "                continue\n",
    "            xs = [p[0] for p in box]; ys = [p[1] for p in box]\n",
    "            cx, cy = sum(xs)/4.0, sum(ys)/4.0\n",
    "            w = (max(xs)-min(xs)) or 1.0; h = (max(ys)-min(ys)) or 1.0\n",
    "            tokens.append(Token(text=txt_clean, conf=float(cf), x=cx, y=cy, w=w, h=h))\n",
    "            lines.append(txt_clean)\n",
    "        return PageResult(tokens=tokens, text=\" \".join(lines), engine=\"easyocr\")\n",
    "\n",
    "# ----------------------------- Candidate Assembly ------------------------------\n",
    "\n",
    "def _sort_by_x(tokens: List[Token]) -> List[Token]:\n",
    "    return sorted(tokens, key=lambda t: (t.y, t.x))\n",
    "\n",
    "def assemble_inline_candidates(neighborhood: List[Token], line_tol: float = 0.85, gap_tol: float = 0.60) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build candidate strings by concatenating adjacent small tokens on the same line:\n",
    "      \"1\" \"-\" \"0\" -> \"1-0\", \"A\" \"A\" -> \"AA\"\n",
    "    Returns de-duplicated strings.\n",
    "    \"\"\"\n",
    "    if not neighborhood:\n",
    "        return []\n",
    "    by_lines: List[List[Token]] = []\n",
    "    toks = _sort_by_x(neighborhood)\n",
    "    for t in toks:\n",
    "        placed = False\n",
    "        for line in by_lines:\n",
    "            anchor = line[0]\n",
    "            same_line = abs(t.y - anchor.y) <= max(anchor.h, t.h) * line_tol\n",
    "            if same_line:\n",
    "                line.append(t); placed = True; break\n",
    "        if not placed:\n",
    "            by_lines.append([t])\n",
    "\n",
    "    cands: set[str] = set()\n",
    "    for line in by_lines:\n",
    "        line = sorted(line, key=lambda t: t.x)\n",
    "        if not line:\n",
    "            continue\n",
    "        avg_h = sum(t.h for t in line) / len(line)\n",
    "        max_gap = avg_h * gap_tol\n",
    "        texts = [norm_val(t.text) for t in line]\n",
    "        xs = [t.x for t in line]\n",
    "        # 2-grams\n",
    "        for i in range(len(line)-1):\n",
    "            if abs(xs[i+1] - xs[i]) <= max_gap:\n",
    "                cands.add(texts[i] + texts[i+1])\n",
    "        # 3-grams\n",
    "        for i in range(len(line)-2):\n",
    "            if abs(xs[i+1] - xs[i]) <= max_gap and abs(xs[i+2] - xs[i+1]) <= max_gap:\n",
    "                cands.add(texts[i] + texts[i+1] + texts[i+2])\n",
    "    return list(cands)\n",
    "\n",
    "# ----------------------------- Scoring (ROI-first then global) -----------------\n",
    "\n",
    "def _nearby_anchor_bonus(tokens_in_zone: List[Token], center_xy: Tuple[float, float], radius=220) -> int:\n",
    "    return sum(1 for a in tokens_in_zone\n",
    "               if norm_val(a.text).upper() in TITLE_ANCHORS and distance((a.x, a.y), center_xy) <= radius)\n",
    "\n",
    "def score_candidates_bottom_right_first(\n",
    "    tokens: List[Token], page_w: float, page_h: float,\n",
    "    brx: float, bry: float, blocklist: Optional[set] = None,\n",
    "    edge_margin: float = DEFAULT_EDGE_MARGIN\n",
    "):\n",
    "    \"\"\"\n",
    "    PASS A (strict, bottom-right only) with:\n",
    "      - edge exclusion (filters page grid letters/numbers),\n",
    "      - neighborhood assembly to recover '1-0' and 'AA',\n",
    "      - pattern-aware scoring preferring N-N and double letters over single letters.\n",
    "    Returns (value, score, center, context) or None.\n",
    "    \"\"\"\n",
    "    block = {t.upper() for t in (blocklist or set())}\n",
    "\n",
    "    # ROI filter + edge exclusion\n",
    "    br_tokens = [\n",
    "        t for t in tokens\n",
    "        if in_bottom_right_strict(t.x, t.y, page_w, page_h, brx, bry)\n",
    "        and is_far_from_edges(t.x, t.y, page_w, page_h, edge_margin)\n",
    "    ]\n",
    "    if not br_tokens:\n",
    "        return None\n",
    "\n",
    "    br_rev_labels = [t for t in br_tokens if REV_TOKEN_RE.match(norm_val(t.text))]\n",
    "\n",
    "    # Priority patterns\n",
    "    def is_hyphen_code(s: str) -> bool:   # e.g., 1-0, 12-01\n",
    "        return bool(re.fullmatch(r\"\\d{1,2}-\\d{1,2}\", s))\n",
    "    def is_double_letter(s: str) -> bool: # AA, AB ...\n",
    "        return bool(re.fullmatch(r\"[A-Z]{2}\", s))\n",
    "    def is_single_letter(s: str) -> bool:\n",
    "        return bool(re.fullmatch(r\"[A-Z]\", s))\n",
    "\n",
    "    def base_score_for(v: str) -> float:\n",
    "        if is_hyphen_code(v):   return 40.0\n",
    "        if is_double_letter(v): return 14.0\n",
    "        if is_single_letter(v): return 4.0\n",
    "        return 8.0\n",
    "\n",
    "    def neighborhood_around(cx: float, cy: float, radius: float = 300.0) -> List[Token]:\n",
    "        return [t for t in br_tokens if distance((t.x, t.y), (cx, cy)) <= radius]\n",
    "\n",
    "    cands: List[Tuple[float, str, Tuple[float,float]]] = []\n",
    "\n",
    "    def consider_token_or_assembled(ref_xy: Tuple[float,float], neigh: List[Token], label_token: Optional[Token]):\n",
    "        # 1) Raw tokens\n",
    "        for t in neigh:\n",
    "            v = norm_val(t.text)\n",
    "            if not REV_VALUE_RE.match(v):\n",
    "                continue\n",
    "            vu = v.upper()\n",
    "            if vu in block:\n",
    "                continue\n",
    "            d = distance((t.x, t.y), ref_xy) + 1e-3\n",
    "            score = base_score_for(v) + 1000.0 / d\n",
    "            if label_token is not None:\n",
    "                if abs(t.y - label_token.y) <= max(label_token.h, t.h) * 0.8:\n",
    "                    score += 6.0\n",
    "                if t.x > label_token.x:\n",
    "                    score += 8.0\n",
    "            if in_bottom_right(t.x, t.y, page_w, page_h): score += 3.0\n",
    "            score += _nearby_anchor_bonus(br_tokens, (t.x, t.y)) * 1.2\n",
    "            cands.append((score, v, (t.x, t.y)))\n",
    "\n",
    "        # 2) Assembled n-grams (recover 1-0, AA, etc.)\n",
    "        assembled = assemble_inline_candidates(neigh, line_tol=0.85, gap_tol=0.60)\n",
    "        for s in assembled:\n",
    "            s_norm = norm_val(s)\n",
    "            if not REV_VALUE_RE.match(s_norm):\n",
    "                continue\n",
    "            if s_norm.upper() in block:\n",
    "                continue\n",
    "            score = base_score_for(s_norm) + 1000.0 / 30.0  # proximity proxy\n",
    "            if label_token is not None:\n",
    "                score += 6.0\n",
    "            cands.append((score, s_norm, ref_xy))\n",
    "\n",
    "    if br_rev_labels:\n",
    "        for r in br_rev_labels:\n",
    "            neigh = neighborhood_around(r.x, r.y, radius=300.0)\n",
    "            consider_token_or_assembled((r.x, r.y), neigh, r)\n",
    "    else:\n",
    "        # Approximate typical REV cell centroid\n",
    "        anchor_xy = (page_w * 0.92, page_h * 0.90)\n",
    "        neigh = neighborhood_around(anchor_xy[0], anchor_xy[1], radius=320.0)\n",
    "        consider_token_or_assembled(anchor_xy, neigh, None)\n",
    "\n",
    "    if not cands:\n",
    "        return None\n",
    "\n",
    "    # If any hyphen-code exists, demote lone single letters harshly\n",
    "    any_hyphen = any(re.fullmatch(r\"\\d{1,2}-\\d{1,2}\", v) for _, v, _ in cands)\n",
    "    if any_hyphen:\n",
    "        cands = [(s - (6.0 if re.fullmatch(r\"[A-Z]\", v) else 0.0), v, xy) for (s, v, xy) in cands]\n",
    "\n",
    "    best = max(cands, key=lambda c: c[0])\n",
    "    score, v, center = best\n",
    "    ctx = context_snippet_from_tokens(tokens, center, radius=160)\n",
    "    return (v, score, center, ctx)\n",
    "\n",
    "def score_candidates_global(tokens: List[Token], page_w: float, page_h: float):\n",
    "    \"\"\"\n",
    "    PASS B (fallback): Global, seeded by any REV label on the page.\n",
    "    Retains down-weights for revision tables and bottom-right bonuses.\n",
    "    \"\"\"\n",
    "    anchor_tokens = [t for t in tokens if norm_val(t.text).upper() in TITLE_ANCHORS]\n",
    "    rev_tokens = [t for t in tokens if REV_TOKEN_RE.match(norm_val(t.text))]\n",
    "    if not rev_tokens:\n",
    "        return None\n",
    "\n",
    "    def nearby_anchor_bonus(center_xy, radius=220):\n",
    "        return sum(1 for a in anchor_tokens if distance((a.x, a.y), center_xy) <= radius)\n",
    "\n",
    "    cands = []\n",
    "    for r in rev_tokens:\n",
    "        r_word = norm_val(r.text).lower()\n",
    "        is_revision_word = r_word.startswith(\"revision\")\n",
    "        neighborhood = [t for t in tokens if distance((t.x, t.y), (r.x, r.y)) <= 280]\n",
    "        looks_like_revision_table = any(norm_val(n.text).upper() in REV_TABLE_HEADERS for n in neighborhood)\n",
    "        for t in neighborhood:\n",
    "            v = norm_val(t.text)\n",
    "            if not REV_VALUE_RE.match(v):\n",
    "                continue\n",
    "            d = distance((t.x, t.y), (r.x, r.y)) + 1e-3\n",
    "            same_line = abs(t.y - r.y) <= max(r.h, t.h) * 0.8\n",
    "            to_right = t.x > r.x\n",
    "            base = 1000.0 / d\n",
    "            if same_line: base += 4.0\n",
    "            if to_right:  base += 6.0\n",
    "            if in_bottom_right(t.x, t.y, page_w, page_h): base += 5.0\n",
    "            base += nearby_anchor_bonus((t.x, t.y)) * 1.5\n",
    "            if t.conf is not None: base += (t.conf - 0.5) * 2.0\n",
    "            if is_revision_word: base -= 2.0\n",
    "            if looks_like_revision_table: base -= 6.0\n",
    "            cands.append((base, v, (t.x, t.y)))\n",
    "\n",
    "    if not cands:\n",
    "        return None\n",
    "\n",
    "    br_cands = [c for c in cands if in_bottom_right(c[2][0], c[2][1], page_w, page_h)]\n",
    "    pool = br_cands if br_cands else cands\n",
    "    score, v, center = max(pool, key=lambda c: c[0])\n",
    "    ctx = context_snippet_from_tokens(tokens, center, radius=160)\n",
    "    return (v, score, center, ctx)\n",
    "\n",
    "# ----------------------------- Rasterization -----------------------------------\n",
    "\n",
    "def rasterize_to_bgr(pdf_path: Path, page_index0: int, dpi: int):\n",
    "    import numpy as np, cv2\n",
    "    try:\n",
    "        from PIL import Image\n",
    "    except Exception:\n",
    "        Image = None\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        page = doc[page_index0]\n",
    "        zoom = dpi / 72.0\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        try:\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False, colorspace=fitz.csRGB)\n",
    "        except TypeError:\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "\n",
    "        buf = getattr(pix, \"samples\", None)\n",
    "        ncomps = getattr(pix, \"n\", None)\n",
    "        try:\n",
    "            if buf and ncomps:\n",
    "                arr = np.frombuffer(buf, dtype=np.uint8)\n",
    "                if ncomps == 3 and arr.size == int(pix.w) * int(pix.h) * 3:\n",
    "                    img_rgb = arr.reshape((pix.h, pix.w, 3))\n",
    "                    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "                if ncomps == 1 and arr.size == int(pix.w) * int(pix.h):\n",
    "                    img_gray = arr.reshape((pix.h, pix.w))\n",
    "                    img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "                if ncomps == 4 and arr.size == int(pix.w) * int(pix.h) * 4:\n",
    "                    img_rgba = arr.reshape((pix.h, pix.w, 4))\n",
    "                    img_bgr = cv2.cvtColor(img_rgba, cv2.COLOR_RGBA2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Robust fallback: PNG decode\n",
    "        try:\n",
    "            png = None\n",
    "            if hasattr(pix, \"tobytes\"):\n",
    "                try:\n",
    "                    png = pix.tobytes(\"png\")\n",
    "                except Exception:\n",
    "                    png = None\n",
    "            if not png and hasattr(pix, \"getPNGData\"):\n",
    "                try:\n",
    "                    png = pix.getPNGData()\n",
    "                except Exception:\n",
    "                    png = None\n",
    "            if png:\n",
    "                arr = np.frombuffer(png, dtype=np.uint8)\n",
    "                img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "                if isinstance(img, np.ndarray):\n",
    "                    return img, float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Last resort: PIL buffer interpretation\n",
    "        try:\n",
    "            if Image is not None:\n",
    "                mode = \"RGB\" if ncomps in (3, None) else (\"L\" if ncomps == 1 else \"RGBA\")\n",
    "                pil = Image.frombytes(mode, (pix.w, pix.h), pix.samples)\n",
    "                arr = np.asarray(pil)\n",
    "                import cv2 as _cv2\n",
    "                if arr.ndim == 2:\n",
    "                    img_bgr = _cv2.cvtColor(arr, _cv2.COLOR_GRAY2BGR)\n",
    "                else:\n",
    "                    img_bgr = _cv2.cvtColor(arr, _cv2.COLOR_RGBA2BGR) if arr.shape[2] == 4 else _cv2.cvtColor(arr, _cv2.COLOR_RGB2BGR)\n",
    "                return img_bgr, float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        raise ValueError(f\"Unable to rasterize {pdf_path.name} p{page_index0+1} to a valid BGR ndarray\")\n",
    "\n",
    "# ----------------------------- Page Analyzers ----------------------------------\n",
    "\n",
    "def analyze_page_native(\n",
    "    pdf_path: Path, page_index0: int, brx: float, bry: float, blocklist: set, edge_margin: float\n",
    ") -> Optional[Tuple[str, str, float, str]]:\n",
    "    \"\"\"\n",
    "    Returns (engine, value, score, context) or None\n",
    "    Engines: 'native_br', 'native', 'native_text'\n",
    "    \"\"\"\n",
    "    native = get_native_tokens(pdf_path, page_index0)\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        pw, ph = doc[page_index0].rect.width, doc[page_index0].rect.height\n",
    "\n",
    "    # Pass A: Strict bottom-right ROI only\n",
    "    if native.tokens:\n",
    "        res = score_candidates_bottom_right_first(native.tokens, pw, ph, brx, bry, blocklist, edge_margin=edge_margin)\n",
    "        if res:\n",
    "            v, score, _, ctx = res\n",
    "            return (\"native_br\", v, score, ctx)\n",
    "\n",
    "    # Pass B: Global fallback (only if Pass A had no hit)\n",
    "    if native.tokens:\n",
    "        res = score_candidates_global(native.tokens, pw, ph)\n",
    "        if res:\n",
    "            v, score, _, ctx = res\n",
    "            return (\"native\", v, score, ctx)\n",
    "\n",
    "    # Lightweight textual fallback if page text was extracted\n",
    "    if native.text:\n",
    "        m = re.search(r\"(?i)\\brev(?:ision)?\\b\\s*[:#\\-]?\\s*([A-Za-z]{1,2}|\\d{1,2}-\\d{1,2})\\b\", native.text)\n",
    "        if m:\n",
    "            return (\"native_text\", norm_val(m.group(1)), 0.3, native.text[:80])\n",
    "\n",
    "    return None\n",
    "\n",
    "def analyze_page_ocr(\n",
    "    pdf_path: Path, page_index0: int, dpi: int,\n",
    "    use_paddle: bool, use_easy: bool,\n",
    "    brx: float, bry: float, blocklist: set, edge_margin: float\n",
    ") -> Optional[Tuple[str, str, float, str]]:\n",
    "    \"\"\"\n",
    "    Returns (engine, value, score, context) or None\n",
    "    Engines: 'paddleocr_br'/'paddleocr', 'easyocr_br'/'easyocr'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_bgr, iw, ih = rasterize_to_bgr(pdf_path, page_index0, dpi)\n",
    "    except Exception as e:\n",
    "        LOG.warning(f\"Rasterization failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    import numpy as _np\n",
    "    if not (isinstance(image_bgr, _np.ndarray) and image_bgr.ndim == 3 and image_bgr.shape[2] == 3 and image_bgr.dtype == _np.uint8):\n",
    "        LOG.warning(f\"Rasterized image invalid for OCR p{page_index0+1} {pdf_path.name}: shape={getattr(image_bgr,'shape',None)} dtype={getattr(image_bgr,'dtype',None)}\")\n",
    "        return None\n",
    "\n",
    "    if use_paddle:\n",
    "        try:\n",
    "            padd = PaddleWrapper().run(image_bgr)\n",
    "            res = score_candidates_bottom_right_first(padd.tokens, iw, ih, brx, bry, blocklist, edge_margin=edge_margin)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"paddleocr_br\", v, score, ctx)\n",
    "            res = score_candidates_global(padd.tokens, iw, ih)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"paddleocr\", v, score, ctx)\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"PaddleOCR failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "\n",
    "    if use_easy:\n",
    "        try:\n",
    "            easy = EasyWrapper().run(image_bgr)\n",
    "            res = score_candidates_bottom_right_first(easy.tokens, iw, ih, brx, bry, blocklist, edge_margin=edge_margin)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"easyocr_br\", v, score, ctx)\n",
    "            res = score_candidates_global(easy.tokens, iw, ih)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"easyocr\", v, score, ctx)\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"EasyOCR failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# ----------------------------- File-Level Processing ---------------------------\n",
    "\n",
    "def _normalize_output_value(v: str) -> str:\n",
    "    \"\"\"\n",
    "    Map special cases:\n",
    "    - 'OF' (exact, case-insensitive) -> 'EMPTY'\n",
    "    - otherwise return normalized v\n",
    "    \"\"\"\n",
    "    vu = norm_val(v).upper()\n",
    "    if vu == \"OF\":\n",
    "        return \"EMPTY\"\n",
    "    return norm_val(v)\n",
    "\n",
    "def process_pdf_native(pdf_path: Path, brx: float, bry: float, blocklist: set, edge_margin: float) -> Optional[RevHit]:\n",
    "    hits: Dict[int, RevHit] = {}\n",
    "    with fitz.open(pdf_path) as d:\n",
    "        n = len(d)\n",
    "    for i in range(n):\n",
    "        res = analyze_page_native(pdf_path, i, brx, bry, blocklist, edge_margin)\n",
    "        if not res:\n",
    "            continue\n",
    "        engine, value, score, ctx = res\n",
    "        page_no = i + 1\n",
    "        prev = hits.get(page_no)\n",
    "        if not prev or score > prev.score:\n",
    "            hits[page_no] = RevHit(file=pdf_path.name, page=page_no, value=value,\n",
    "                                   engine=engine, score=score, context_snippet=ctx)\n",
    "    if not hits:\n",
    "        return None\n",
    "    best = max(hits.values(), key=lambda h: getattr(h, 'score', 0))\n",
    "    return best\n",
    "\n",
    "def process_pdf_ocr(pdf_path: Path, dpi: int, use_paddle: bool, use_easy: bool,\n",
    "                    brx: float, bry: float, blocklist: set, edge_margin: float) -> Optional[RevHit]:\n",
    "    hits: Dict[int, RevHit] = {}\n",
    "    with fitz.open(pdf_path) as d:\n",
    "        n = len(d)\n",
    "    for i in range(n):\n",
    "        res = analyze_page_ocr(pdf_path, i, dpi, use_paddle, use_easy, brx, bry, blocklist, edge_margin)\n",
    "        if not res:\n",
    "            continue\n",
    "        engine, value, score, ctx = res\n",
    "        page_no = i + 1\n",
    "        prev = hits.get(page_no)\n",
    "        if not prev or score > prev.score:\n",
    "            hits[page_no] = RevHit(file=pdf_path.name, page=page_no, value=value,\n",
    "                                   engine=engine, score=score, context_snippet=ctx)\n",
    "    if not hits:\n",
    "        return None\n",
    "    best = max(hits.values(), key=lambda h: getattr(h, 'score', 0))\n",
    "    return best\n",
    "\n",
    "def iter_pdfs(folder: Path) -> Iterable[Path]:\n",
    "    # Avoid duplicates; yield .pdf files only\n",
    "    seen = set()\n",
    "    for p in folder.iterdir():\n",
    "        try:\n",
    "            if p.is_file() and p.suffix.lower() == \".pdf\":\n",
    "                rp = p.resolve()\n",
    "                if rp not in seen:\n",
    "                    seen.add(rp)\n",
    "                    yield p\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# ----------------------------- Pipeline (Native → OCR) -------------------------\n",
    "\n",
    "def run_pipeline(input_folder: Path, output_csv: Path, dpi: int,\n",
    "                 enable_paddle: bool, enable_easy: bool,\n",
    "                 brx: float, bry: float, rev_2l_blocklist: set,\n",
    "                 edge_margin: float) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    pdfs = list(iter_pdfs(input_folder))\n",
    "    if not pdfs:\n",
    "        LOG.warning(f\"No PDFs found in {input_folder}\")\n",
    "\n",
    "    for p in tqdm(pdfs, desc=\"Scanning PDFs\"):\n",
    "        try:\n",
    "            # 1) Native pass (strict ROI, then global)\n",
    "            native_best = process_pdf_native(p, brx, bry, rev_2l_blocklist, edge_margin)\n",
    "\n",
    "            if native_best:\n",
    "                value = _normalize_output_value(native_best.value)\n",
    "                rows.append({\"file\": p.name, \"value\": value, \"engine\": native_best.engine})\n",
    "                continue  # Do NOT OCR this file\n",
    "\n",
    "            # 2) OCR fallback (only if native produced nothing)\n",
    "            ocr_best = process_pdf_ocr(p, dpi, enable_paddle, enable_easy, brx, bry, rev_2l_blocklist, edge_margin)\n",
    "            if ocr_best:\n",
    "                value = _normalize_output_value(ocr_best.value)\n",
    "                rows.append({\"file\": p.name, \"value\": value, \"engine\": ocr_best.engine})\n",
    "                continue\n",
    "\n",
    "            # 3) Neither pipeline produced a value → blank\n",
    "            rows.append({\"file\": p.name, \"value\": \"\", \"engine\": \"\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"Failed {p.name}: {e}\")\n",
    "            rows.append({\"file\": p.name, \"value\": \"\", \"engine\": \"\"})\n",
    "\n",
    "    # --- Write CSV (exactly one row per file, 3 columns) ---\n",
    "    try:\n",
    "        output_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as outf:\n",
    "            writer = csv.writer(outf)\n",
    "            writer.writerow(['file', 'value', 'engine'])\n",
    "            for r in rows:\n",
    "                fs = _scalarize(r.get('file', ''))\n",
    "                vs = _scalarize(r.get('value', ''))\n",
    "                es = _scalarize(r.get('engine', ''))\n",
    "                writer.writerow([fs, vs, es])\n",
    "        LOG.info(f\"Wrote CSV to {output_csv.resolve()} with {len(rows)} rows\")\n",
    "    except Exception as e:\n",
    "        LOG.error(f\"Failed to write CSV: {e}\")\n",
    "\n",
    "    return rows\n",
    "\n",
    "# ----------------------------- CLI --------------------------------------------\n",
    "\n",
    "def parse_args(argv=None):\n",
    "    a = argparse.ArgumentParser(description=\"Extract REV values (bottom-right-first).\")\n",
    "    a.add_argument(\"input_folder\", type=Path)\n",
    "    a.add_argument(\"-o\",\"--output\", type=Path, default=Path(\"rev_results.csv\"))\n",
    "    a.add_argument(\"--dpi\", type=int, default=240, help=\"OCR rasterization DPI\")\n",
    "    a.add_argument(\"--no-paddle\", action=\"store_true\", help=\"Disable PaddleOCR\")\n",
    "    a.add_argument(\"--no-easy\", action=\"store_true\", help=\"Disable EasyOCR\")\n",
    "    a.add_argument(\"--br-x\", type=float, default=DEFAULT_BR_X,\n",
    "                   help=\"Bottom-right ROI X ratio (default 0.68)\")\n",
    "    a.add_argument(\"--br-y\", type=float, default=DEFAULT_BR_Y,\n",
    "                   help=\"Bottom-right ROI Y ratio (default 0.72)\")\n",
    "    a.add_argument(\"--edge-margin\", type=float, default=DEFAULT_EDGE_MARGIN,\n",
    "                   help=\"Fraction of page dims to ignore near edges (default 0.018)\")\n",
    "    a.add_argument(\"--rev-2l-blocklist\", type=str,\n",
    "                   default=\",\".join(sorted(DEFAULT_REV_2L_BLOCKLIST)),\n",
    "                   help=\"Comma-separated two-letter tokens to ignore as REV in ROI (OF is intentionally allowed).\")\n",
    "    return a.parse_args(argv)\n",
    "\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "    blocklist = {s.strip().upper() for s in args.rev_2l_blocklist.split(\",\") if s.strip()}\n",
    "    return run_pipeline(\n",
    "        input_folder=args.input_folder,\n",
    "        output_csv=args.output,\n",
    "        dpi=args.dpi,\n",
    "        enable_paddle=not args.no_paddle,\n",
    "        enable_easy=not args.no_easy,\n",
    "        brx=args.br_x,\n",
    "        bry=args.br_y,\n",
    "        rev_2l_blocklist=blocklist,\n",
    "        edge_margin=args.edge_margin\n",
    "    )\n",
    "\n",
    "def _in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\" and not _in_notebook():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d220ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba673c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "REV Extractor — Bottom-Right-First (Native → OCR fallback)\n",
    "Outputs exactly one row per input PDF: file, value, engine\n",
    "- Native vector text pass first; only files with no native hit fall back to OCR\n",
    "- Strict bottom-right ROI prioritization (title block)\n",
    "- 'OF' inside ROI mapped to 'EMPTY'\n",
    "- Fix for top-right leakage (e.g., 'DF'): ROI pass now returns 'OF' from bottom-right\n",
    "  when REV cell is empty, preventing global fallback from selecting top-right tokens.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse, logging, re, math, csv\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "LOG = logging.getLogger(\"rev_extractor_br_first\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "# ----------------------------- Patterns & Constants -----------------------------\n",
    "\n",
    "# Allowed REV value patterns:\n",
    "# - 1–2 letters (A..Z or AA)\n",
    "# - hyphenated numeric pair (e.g., 1-0, 12-01)\n",
    "REV_VALUE_RE = re.compile(r\"^(?:[A-Z]{1,2}|\\d{1,2}-\\d{1,2})$\")\n",
    "\n",
    "# Tokens to detect a REV label\n",
    "REV_TOKEN_RE = re.compile(r\"^rev\\.?$\", re.IGNORECASE)\n",
    "\n",
    "# Weak anchors often present near title blocks\n",
    "TITLE_ANCHORS = {\"DWG\", \"DWG.\", \"DWGNO\", \"SHEET\", \"SCALE\", \"WEIGHT\", \"SIZE\", \"TITLE\"}\n",
    "\n",
    "# Headers around revision tables (down-weighted in global fallback)\n",
    "REV_TABLE_HEADERS = {\"REVISIONS\", \"DESCRIPTION\", \"EC\", \"DFT\", \"APPR\", \"APPD\", \"DATE\", \"CHKD\", \"DRAWN\"}\n",
    "\n",
    "# ROI defaults for bottom-right title block (can be adjusted via CLI)\n",
    "DEFAULT_BR_X = 0.68\n",
    "DEFAULT_BR_Y = 0.72\n",
    "\n",
    "# Edge margin to exclude tokens too close to page borders (grid letters/numbers)\n",
    "DEFAULT_EDGE_MARGIN = 0.018  # ~1.8% of page width/height (tune 0.015–0.025)\n",
    "\n",
    "# Two-letter junk frequently seen in title blocks; keep 'OF' allowed (used to infer EMPTY)\n",
    "DEFAULT_REV_2L_BLOCKLIST = {\"EC\", \"DF\", \"DT\", \"AP\", \"ID\", \"NO\", \"IN\", \"ON\", \"BY\"}\n",
    "\n",
    "# ----------------------------- Data Structures ---------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    text: str\n",
    "    conf: Optional[float]\n",
    "    x: float\n",
    "    y: float\n",
    "    w: float\n",
    "    h: float\n",
    "\n",
    "@dataclass\n",
    "class PageResult:\n",
    "    tokens: List[Token]\n",
    "    text: str\n",
    "    engine: str\n",
    "\n",
    "@dataclass\n",
    "class RevHit:\n",
    "    file: str\n",
    "    page: int\n",
    "    value: str\n",
    "    engine: str\n",
    "    score: float\n",
    "    context_snippet: str\n",
    "\n",
    "# ----------------------------- Utilities ---------------------------------------\n",
    "\n",
    "def _scalarize(v: Any):\n",
    "    \"\"\"Coerce any non-scalar to a plain Python scalar or string.\"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(v, np.ndarray):\n",
    "            return \", \".join(map(str, v.flatten().tolist()))\n",
    "        if isinstance(v, np.generic):\n",
    "            try:\n",
    "                return v.item()\n",
    "            except Exception:\n",
    "                return str(v)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(v, (list, tuple, set)):\n",
    "        return \", \".join(map(str, v))\n",
    "    if isinstance(v, dict):\n",
    "        return \", \".join(f\"{k}={str(vv)}\" for k, vv in v.items())\n",
    "    if isinstance(v, (bytes, bytearray)):\n",
    "        return v.decode(\"utf-8\", errors=\"ignore\")\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(v, (np.integer, np.floating, np.bool_)):\n",
    "            return v.item()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(v, (str, int, float, bool)):\n",
    "        return v\n",
    "    return str(v)\n",
    "\n",
    "def norm_val(v: Any) -> str:\n",
    "    \"\"\"Normalize token text for comparisons.\"\"\"\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    s = str(v).replace(\"\\u00A0\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def in_bottom_right(x: float, y: float, width: float, height: float) -> bool:\n",
    "    return x > width * 0.55 and y > height * 0.60\n",
    "\n",
    "def in_bottom_right_strict(x: float, y: float, width: float, height: float, brx: float, bry: float) -> bool:\n",
    "    return x >= width * brx and y >= height * bry\n",
    "\n",
    "def is_far_from_edges(x: float, y: float, width: float, height: float, edge_margin: float) -> bool:\n",
    "    \"\"\"Filter out tokens too close to page edges (removes border grid labels like K/L/16).\"\"\"\n",
    "    xm = width * edge_margin\n",
    "    ym = height * edge_margin\n",
    "    return (x > xm) and (x < width - xm) and (y > ym) and (y < height - ym)\n",
    "\n",
    "def distance(a: Tuple[float, float], b: Tuple[float, float]) -> float:\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def context_snippet_from_tokens(tokens: List[Token], center: Tuple[float, float], radius: float = 160) -> str:\n",
    "    close = [t.text for t in tokens if distance((t.x, t.y), center) <= radius]\n",
    "    s = \" \".join(close)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s[:80]\n",
    "\n",
    "# ----------------------------- Native Tokenization ------------------------------\n",
    "\n",
    "def get_native_tokens(pdf_path: Path, page_index0: int) -> PageResult:\n",
    "    tokens: List[Token] = []\n",
    "    text_parts: List[str] = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        page = doc[page_index0]\n",
    "        for x0, y0, x1, y1, txt, *_ in page.get_text(\"words\"):\n",
    "            txt_clean = txt.strip()\n",
    "            if not txt_clean:\n",
    "                continue\n",
    "            cx = (x0 + x1) / 2.0\n",
    "            cy = (y0 + y1) / 2.0\n",
    "            tokens.append(Token(text=txt_clean, conf=None, x=cx, y=cy, w=(x1-x0), h=(y1-y0)))\n",
    "            text_parts.append(txt_clean)\n",
    "    return PageResult(tokens=tokens, text=\" \".join(text_parts), engine=\"native\")\n",
    "\n",
    "# ----------------------------- OCR Wrappers ------------------------------------\n",
    "\n",
    "class PaddleWrapper:\n",
    "    def __init__(self):\n",
    "        from paddleocr import PaddleOCR\n",
    "        self.ocr = PaddleOCR(lang=\"en\", use_angle_cls=True, show_log=False)\n",
    "\n",
    "    def run(self, image_bgr):\n",
    "        result = self.ocr.ocr(image_bgr, cls=True)\n",
    "        tokens: List[Token] = []\n",
    "        lines: List[str] = []\n",
    "        for det in result:\n",
    "            for (box, (txt, cf)) in det:\n",
    "                txt_clean = txt.strip()\n",
    "                if not txt_clean:\n",
    "                    continue\n",
    "                xs = [p[0] for p in box]; ys = [p[1] for p in box]\n",
    "                cx, cy = sum(xs)/4.0, sum(ys)/4.0\n",
    "                w = (max(xs)-min(xs)) or 1.0; h = (max(ys)-min(ys)) or 1.0\n",
    "                tokens.append(Token(text=txt_clean, conf=float(cf), x=cx, y=cy, w=w, h=h))\n",
    "                lines.append(txt_clean)\n",
    "        return PageResult(tokens=tokens, text=\" \".join(lines), engine=\"paddleocr\")\n",
    "\n",
    "class EasyWrapper:\n",
    "    def __init__(self):\n",
    "        import easyocr\n",
    "        self.reader = easyocr.Reader([\"en\"], gpu=False)\n",
    "\n",
    "    def run(self, image_bgr):\n",
    "        result = self.reader.readtext(image_bgr)\n",
    "        tokens: List[Token] = []\n",
    "        lines: List[str] = []\n",
    "        for (box, txt, cf) in result:\n",
    "            txt_clean = txt.strip()\n",
    "            if not txt_clean:\n",
    "                continue\n",
    "            xs = [p[0] for p in box]; ys = [p[1] for p in box]\n",
    "            cx, cy = sum(xs)/4.0, sum(ys)/4.0\n",
    "            w = (max(xs)-min(xs)) or 1.0; h = (max(ys)-min(ys)) or 1.0\n",
    "            tokens.append(Token(text=txt_clean, conf=float(cf), x=cx, y=cy, w=w, h=h))\n",
    "            lines.append(txt_clean)\n",
    "        return PageResult(tokens=tokens, text=\" \".join(lines), engine=\"easyocr\")\n",
    "\n",
    "# ----------------------------- Candidate Assembly ------------------------------\n",
    "\n",
    "def _sort_by_x(tokens: List[Token]) -> List[Token]:\n",
    "    return sorted(tokens, key=lambda t: (t.y, t.x))\n",
    "\n",
    "def assemble_inline_candidates(neighborhood: List[Token], line_tol: float = 0.85, gap_tol: float = 0.60) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build candidate strings by concatenating adjacent small tokens on the same line:\n",
    "      \"1\" \"-\" \"0\" -> \"1-0\", \"A\" \"A\" -> \"AA\"\n",
    "    Returns de-duplicated strings.\n",
    "    \"\"\"\n",
    "    if not neighborhood:\n",
    "        return []\n",
    "    by_lines: List[List[Token]] = []\n",
    "    toks = _sort_by_x(neighborhood)\n",
    "    for t in toks:\n",
    "        placed = False\n",
    "        for line in by_lines:\n",
    "            anchor = line[0]\n",
    "            same_line = abs(t.y - anchor.y) <= max(anchor.h, t.h) * line_tol\n",
    "            if same_line:\n",
    "                line.append(t); placed = True; break\n",
    "        if not placed:\n",
    "            by_lines.append([t])\n",
    "\n",
    "    cands: set[str] = set()\n",
    "    for line in by_lines:\n",
    "        line = sorted(line, key=lambda t: t.x)\n",
    "        if not line:\n",
    "            continue\n",
    "        avg_h = sum(t.h for t in line) / len(line)\n",
    "        max_gap = avg_h * gap_tol\n",
    "        texts = [norm_val(t.text) for t in line]\n",
    "        xs = [t.x for t in line]\n",
    "        # 2-grams\n",
    "        for i in range(len(line)-1):\n",
    "            if abs(xs[i+1] - xs[i]) <= max_gap:\n",
    "                cands.add(texts[i] + texts[i+1])\n",
    "        # 3-grams\n",
    "        for i in range(len(line)-2):\n",
    "            if abs(xs[i+1] - xs[i]) <= max_gap and abs(xs[i+2] - xs[i+1]) <= max_gap:\n",
    "                cands.add(texts[i] + texts[i+1] + texts[i+2])\n",
    "    return list(cands)\n",
    "\n",
    "# ----------------------------- Scoring (ROI-first then global) -----------------\n",
    "\n",
    "def _nearby_anchor_bonus(tokens_in_zone: List[Token], center_xy: Tuple[float, float], radius=220) -> int:\n",
    "    return sum(1 for a in tokens_in_zone\n",
    "               if norm_val(a.text).upper() in TITLE_ANCHORS and distance((a.x, a.y), center_xy) <= radius)\n",
    "\n",
    "def _return_of_if_present(br_tokens: List[Token], all_tokens: List[Token]) -> Optional[Tuple[str, float, Tuple[float,float], str]]:\n",
    "    \"\"\"\n",
    "    Last-resort sentinel for empty REV cells:\n",
    "      If there's a standalone 'OF' token inside the bottom-right ROI,\n",
    "      return it with a tiny score so the ROI pass succeeds and global fallback is never used.\n",
    "    \"\"\"\n",
    "    for t in br_tokens:\n",
    "        if norm_val(t.text).upper() == \"OF\":\n",
    "            center = (t.x, t.y)\n",
    "            ctx = context_snippet_from_tokens(all_tokens, center, radius=160)\n",
    "            return (\"OF\", 0.05, center, ctx)\n",
    "    return None\n",
    "\n",
    "def score_candidates_bottom_right_first(\n",
    "    tokens: List[Token], page_w: float, page_h: float,\n",
    "    brx: float, bry: float, blocklist: Optional[set] = None,\n",
    "    edge_margin: float = DEFAULT_EDGE_MARGIN\n",
    "):\n",
    "    \"\"\"\n",
    "    PASS A (strict, bottom-right only) with:\n",
    "      - edge exclusion (filters page grid letters/numbers),\n",
    "      - neighborhood assembly to recover '1-0' and 'AA',\n",
    "      - pattern-aware scoring preferring N-N and double letters over single letters.\n",
    "      - **NEW**: if no candidate found, but 'OF' exists in ROI, return 'OF' from ROI (sentinel for EMPTY).\n",
    "    Returns (value, score, center, context) or None.\n",
    "    \"\"\"\n",
    "    block = {t.upper() for t in (blocklist or set())}\n",
    "\n",
    "    # ROI filter + edge exclusion\n",
    "    br_tokens = [\n",
    "        t for t in tokens\n",
    "        if in_bottom_right_strict(t.x, t.y, page_w, page_h, brx, bry)\n",
    "        and is_far_from_edges(t.x, t.y, page_w, page_h, edge_margin)\n",
    "    ]\n",
    "    if not br_tokens:\n",
    "        return None\n",
    "\n",
    "    br_rev_labels = [t for t in br_tokens if REV_TOKEN_RE.match(norm_val(t.text))]\n",
    "\n",
    "    # Priority patterns\n",
    "    def is_hyphen_code(s: str) -> bool:   # e.g., 1-0, 12-01\n",
    "        return bool(re.fullmatch(r\"\\d{1,2}-\\d{1,2}\", s))\n",
    "    def is_double_letter(s: str) -> bool: # AA, AB ...\n",
    "        return bool(re.fullmatch(r\"[A-Z]{2}\", s))\n",
    "    def is_single_letter(s: str) -> bool:\n",
    "        return bool(re.fullmatch(r\"[A-Z]\", s))\n",
    "\n",
    "    def base_score_for(v: str) -> float:\n",
    "        if is_hyphen_code(v):   return 40.0\n",
    "        if is_double_letter(v): return 14.0\n",
    "        if is_single_letter(v): return 4.0\n",
    "        return 8.0\n",
    "\n",
    "    def neighborhood_around(cx: float, cy: float, radius: float = 300.0) -> List[Token]:\n",
    "        return [t for t in br_tokens if distance((t.x, t.y), (cx, cy)) <= radius]\n",
    "\n",
    "    cands: List[Tuple[float, str, Tuple[float,float]]] = []\n",
    "\n",
    "    def consider_token_or_assembled(ref_xy: Tuple[float,float], neigh: List[Token], label_token: Optional[Token]):\n",
    "        # 1) Raw tokens\n",
    "        for t in neigh:\n",
    "            v = norm_val(t.text)\n",
    "            if not REV_VALUE_RE.match(v):\n",
    "                continue\n",
    "            vu = v.upper()\n",
    "            if vu in block:\n",
    "                continue\n",
    "            d = distance((t.x, t.y), ref_xy) + 1e-3\n",
    "            score = base_score_for(v) + 1000.0 / d\n",
    "            if label_token is not None:\n",
    "                if abs(t.y - label_token.y) <= max(label_token.h, t.h) * 0.8:\n",
    "                    score += 6.0\n",
    "                if t.x > label_token.x:\n",
    "                    score += 8.0\n",
    "            if in_bottom_right(t.x, t.y, page_w, page_h): score += 3.0\n",
    "            score += _nearby_anchor_bonus(br_tokens, (t.x, t.y)) * 1.2\n",
    "            cands.append((score, v, (t.x, t.y)))\n",
    "\n",
    "        # 2) Assembled n-grams (recover 1-0, AA, etc.)\n",
    "        assembled = assemble_inline_candidates(neigh, line_tol=0.85, gap_tol=0.60)\n",
    "        for s in assembled:\n",
    "            s_norm = norm_val(s)\n",
    "            if not REV_VALUE_RE.match(s_norm):\n",
    "                continue\n",
    "            if s_norm.upper() in block:\n",
    "                continue\n",
    "            score = base_score_for(s_norm) + 1000.0 / 30.0  # proximity proxy\n",
    "            if label_token is not None:\n",
    "                score += 6.0\n",
    "            cands.append((score, s_norm, ref_xy))\n",
    "\n",
    "    if br_rev_labels:\n",
    "        for r in br_rev_labels:\n",
    "            neigh = neighborhood_around(r.x, r.y, radius=300.0)\n",
    "            consider_token_or_assembled((r.x, r.y), neigh, r)\n",
    "    else:\n",
    "        # Approximate typical REV cell centroid\n",
    "        anchor_xy = (page_w * 0.92, page_h * 0.90)\n",
    "        neigh = neighborhood_around(anchor_xy[0], anchor_xy[1], radius=320.0)\n",
    "        consider_token_or_assembled(anchor_xy, neigh, None)\n",
    "\n",
    "    if not cands:\n",
    "        # NEW: last-resort ROI sentinel – if 'OF' present in ROI, return it to prevent global leakage\n",
    "        of_hit = _return_of_if_present(br_tokens, tokens)\n",
    "        if of_hit is not None:\n",
    "            v, score, center, ctx = of_hit\n",
    "            return (v, score, center, ctx)\n",
    "        return None\n",
    "\n",
    "    # If any hyphen-code exists, demote lone single letters harshly\n",
    "    any_hyphen = any(re.fullmatch(r\"\\d{1,2}-\\d{1,2}\", v) for _, v, _ in cands)\n",
    "    if any_hyphen:\n",
    "        cands = [(s - (6.0 if re.fullmatch(r\"[A-Z]\", v) else 0.0), v, xy) for (s, v, xy) in cands]\n",
    "\n",
    "    best = max(cands, key=lambda c: c[0])\n",
    "    score, v, center = best\n",
    "    ctx = context_snippet_from_tokens(tokens, center, radius=160)\n",
    "    return (v, score, center, ctx)\n",
    "\n",
    "def score_candidates_global(tokens: List[Token], page_w: float, page_h: float):\n",
    "    \"\"\"\n",
    "    PASS B (fallback): Global, seeded by any REV label on the page.\n",
    "    Retains down-weights for revision tables and bottom-right bonuses.\n",
    "    \"\"\"\n",
    "    anchor_tokens = [t for t in tokens if norm_val(t.text).upper() in TITLE_ANCHORS]\n",
    "    rev_tokens = [t for t in tokens if REV_TOKEN_RE.match(norm_val(t.text))]\n",
    "    if not rev_tokens:\n",
    "        return None\n",
    "\n",
    "    def nearby_anchor_bonus(center_xy, radius=220):\n",
    "        return sum(1 for a in anchor_tokens if distance((a.x, a.y), center_xy) <= radius)\n",
    "\n",
    "    cands = []\n",
    "    for r in rev_tokens:\n",
    "        r_word = norm_val(r.text).lower()\n",
    "        is_revision_word = r_word.startswith(\"revision\")\n",
    "        neighborhood = [t for t in tokens if distance((t.x, t.y), (r.x, r.y)) <= 280]\n",
    "        looks_like_revision_table = any(norm_val(n.text).upper() in REV_TABLE_HEADERS for n in neighborhood)\n",
    "        for t in neighborhood:\n",
    "            v = norm_val(t.text)\n",
    "            if not REV_VALUE_RE.match(v):\n",
    "                continue\n",
    "            d = distance((t.x, t.y), (r.x, r.y)) + 1e-3\n",
    "            same_line = abs(t.y - r.y) <= max(r.h, t.h) * 0.8\n",
    "            to_right = t.x > r.x\n",
    "            base = 1000.0 / d\n",
    "            if same_line: base += 4.0\n",
    "            if to_right:  base += 6.0\n",
    "            if in_bottom_right(t.x, t.y, page_w, page_h): base += 5.0\n",
    "            base += nearby_anchor_bonus((t.x, t.y)) * 1.5\n",
    "            if t.conf is not None: base += (t.conf - 0.5) * 2.0\n",
    "            if is_revision_word: base -= 2.0\n",
    "            if looks_like_revision_table: base -= 6.0\n",
    "            cands.append((base, v, (t.x, t.y)))\n",
    "\n",
    "    if not cands:\n",
    "        return None\n",
    "\n",
    "    br_cands = [c for c in cands if in_bottom_right(c[2][0], c[2][1], page_w, page_h)]\n",
    "    pool = br_cands if br_cands else cands\n",
    "    score, v, center = max(pool, key=lambda c: c[0])\n",
    "    ctx = context_snippet_from_tokens(tokens, center, radius=160)\n",
    "    return (v, score, center, ctx)\n",
    "\n",
    "# ----------------------------- Rasterization -----------------------------------\n",
    "\n",
    "def rasterize_to_bgr(pdf_path: Path, page_index0: int, dpi: int):\n",
    "    import numpy as np, cv2\n",
    "    try:\n",
    "        from PIL import Image\n",
    "    except Exception:\n",
    "        Image = None\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        page = doc[page_index0]\n",
    "        zoom = dpi / 72.0\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        try:\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False, colorspace=fitz.csRGB)\n",
    "        except TypeError:\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "\n",
    "        buf = getattr(pix, \"samples\", None)\n",
    "        ncomps = getattr(pix, \"n\", None)\n",
    "        try:\n",
    "            if buf and ncomps:\n",
    "                arr = np.frombuffer(buf, dtype=np.uint8)\n",
    "                if ncomps == 3 and arr.size == int(pix.w) * int(pix.h) * 3:\n",
    "                    img_rgb = arr.reshape((pix.h, pix.w, 3))\n",
    "                    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "                if ncomps == 1 and arr.size == int(pix.w) * int(pix.h):\n",
    "                    img_gray = arr.reshape((pix.h, pix.w))\n",
    "                    img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "                if ncomps == 4 and arr.size == int(pix.w) * int(pix.h) * 4:\n",
    "                    img_rgba = arr.reshape((pix.h, pix.w, 4))\n",
    "                    img_bgr = cv2.cvtColor(img_rgba, cv2.COLOR_RGBA2BGR)\n",
    "                    return img_bgr, float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Robust fallback: PNG decode\n",
    "        try:\n",
    "            png = None\n",
    "            if hasattr(pix, \"tobytes\"):\n",
    "                try:\n",
    "                    png = pix.tobytes(\"png\")\n",
    "                except Exception:\n",
    "                    png = None\n",
    "            if not png and hasattr(pix, \"getPNGData\"):\n",
    "                try:\n",
    "                    png = pix.getPNGData()\n",
    "                except Exception:\n",
    "                    png = None\n",
    "            if png:\n",
    "                arr = np.frombuffer(png, dtype=np.uint8)\n",
    "                img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "                if isinstance(img, np.ndarray):\n",
    "                    return img, float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Last resort: PIL buffer interpretation\n",
    "        try:\n",
    "            if Image is not None:\n",
    "                mode = \"RGB\" if ncomps in (3, None) else (\"L\" if ncomps == 1 else \"RGBA\")\n",
    "                pil = Image.frombytes(mode, (pix.w, pix.h), pix.samples)\n",
    "                arr = np.asarray(pil)\n",
    "                import cv2 as _cv2\n",
    "                if arr.ndim == 2:\n",
    "                    img_bgr = _cv2.cvtColor(arr, _cv2.COLOR_GRAY2BGR)\n",
    "                else:\n",
    "                    img_bgr = _cv2.cvtColor(arr, _cv2.COLOR_RGBA2BGR) if arr.shape[2] == 4 else _cv2.cvtColor(arr, _cv2.COLOR_RGB2BGR)\n",
    "                return img_bgr, float(pix.w), float(pix.h)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        raise ValueError(f\"Unable to rasterize {pdf_path.name} p{page_index0+1} to a valid BGR ndarray\")\n",
    "\n",
    "# ----------------------------- Page Analyzers ----------------------------------\n",
    "\n",
    "def analyze_page_native(\n",
    "    pdf_path: Path, page_index0: int, brx: float, bry: float, blocklist: set, edge_margin: float\n",
    ") -> Optional[Tuple[str, str, float, str]]:\n",
    "    \"\"\"\n",
    "    Returns (engine, value, score, context) or None\n",
    "    Engines: 'native_br', 'native', 'native_text'\n",
    "    \"\"\"\n",
    "    native = get_native_tokens(pdf_path, page_index0)\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        pw, ph = doc[page_index0].rect.width, doc[page_index0].rect.height\n",
    "\n",
    "    # Pass A: Strict bottom-right ROI only\n",
    "    if native.tokens:\n",
    "        res = score_candidates_bottom_right_first(native.tokens, pw, ph, brx, bry, blocklist, edge_margin=edge_margin)\n",
    "        if res:\n",
    "            v, score, _, ctx = res\n",
    "            return (\"native_br\", v, score, ctx)\n",
    "\n",
    "    # Pass B: Global fallback (only if Pass A had no hit)\n",
    "    if native.tokens:\n",
    "        res = score_candidates_global(native.tokens, pw, ph)\n",
    "        if res:\n",
    "            v, score, _, ctx = res\n",
    "            return (\"native\", v, score, ctx)\n",
    "\n",
    "    # Lightweight textual fallback if page text was extracted\n",
    "    if native.text:\n",
    "        m = re.search(r\"(?i)\\brev(?:ision)?\\b\\s*[:#\\-]?\\s*([A-Za-z]{1,2}|\\d{1,2}-\\d{1,2})\\b\", native.text)\n",
    "        if m:\n",
    "            return (\"native_text\", norm_val(m.group(1)), 0.3, native.text[:80])\n",
    "\n",
    "    return None\n",
    "\n",
    "def analyze_page_ocr(\n",
    "    pdf_path: Path, page_index0: int, dpi: int,\n",
    "    use_paddle: bool, use_easy: bool,\n",
    "    brx: float, bry: float, blocklist: set, edge_margin: float\n",
    ") -> Optional[Tuple[str, str, float, str]]:\n",
    "    \"\"\"\n",
    "    Returns (engine, value, score, context) or None\n",
    "    Engines: 'paddleocr_br'/'paddleocr', 'easyocr_br'/'easyocr'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_bgr, iw, ih = rasterize_to_bgr(pdf_path, page_index0, dpi)\n",
    "    except Exception as e:\n",
    "        LOG.warning(f\"Rasterization failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    import numpy as _np\n",
    "    if not (isinstance(image_bgr, _np.ndarray) and image_bgr.ndim == 3 and image_bgr.shape[2] == 3 and image_bgr.dtype == _np.uint8):\n",
    "        LOG.warning(f\"Rasterized image invalid for OCR p{page_index0+1} {pdf_path.name}: shape={getattr(image_bgr,'shape',None)} dtype={getattr(image_bgr,'dtype',None)}\")\n",
    "        return None\n",
    "\n",
    "    if use_paddle:\n",
    "        try:\n",
    "            padd = PaddleWrapper().run(image_bgr)\n",
    "            res = score_candidates_bottom_right_first(padd.tokens, iw, ih, brx, bry, blocklist, edge_margin=edge_margin)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"paddleocr_br\", v, score, ctx)\n",
    "            res = score_candidates_global(padd.tokens, iw, ih)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"paddleocr\", v, score, ctx)\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"PaddleOCR failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "\n",
    "    if use_easy:\n",
    "        try:\n",
    "            easy = EasyWrapper().run(image_bgr)\n",
    "            res = score_candidates_bottom_right_first(easy.tokens, iw, ih, brx, bry, blocklist, edge_margin=edge_margin)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"easyocr_br\", v, score, ctx)\n",
    "            res = score_candidates_global(easy.tokens, iw, ih)\n",
    "            if res:\n",
    "                v, score, _, ctx = res\n",
    "                return (\"easyocr\", v, score, ctx)\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"EasyOCR failed p{page_index0+1} {pdf_path.name}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# ----------------------------- File-Level Processing ---------------------------\n",
    "\n",
    "def _normalize_output_value(v: str) -> str:\n",
    "    \"\"\"\n",
    "    Map special cases:\n",
    "    - 'OF' (exact, case-insensitive) -> 'EMPTY'\n",
    "    - otherwise return normalized v\n",
    "    \"\"\"\n",
    "    vu = norm_val(v).upper()\n",
    "    if vu == \"OF\":\n",
    "        return \"EMPTY\"\n",
    "    return norm_val(v)\n",
    "\n",
    "def process_pdf_native(pdf_path: Path, brx: float, bry: float, blocklist: set, edge_margin: float) -> Optional[RevHit]:\n",
    "    hits: Dict[int, RevHit] = {}\n",
    "    with fitz.open(pdf_path) as d:\n",
    "        n = len(d)\n",
    "    for i in range(n):\n",
    "        res = analyze_page_native(pdf_path, i, brx, bry, blocklist, edge_margin)\n",
    "        if not res:\n",
    "            continue\n",
    "        engine, value, score, ctx = res\n",
    "        page_no = i + 1\n",
    "        prev = hits.get(page_no)\n",
    "        if not prev or score > prev.score:\n",
    "            hits[page_no] = RevHit(file=pdf_path.name, page=page_no, value=value,\n",
    "                                   engine=engine, score=score, context_snippet=ctx)\n",
    "    if not hits:\n",
    "        return None\n",
    "    best = max(hits.values(), key=lambda h: getattr(h, 'score', 0))\n",
    "    return best\n",
    "\n",
    "def process_pdf_ocr(pdf_path: Path, dpi: int, use_paddle: bool, use_easy: bool,\n",
    "                    brx: float, bry: float, blocklist: set, edge_margin: float) -> Optional[RevHit]:\n",
    "    hits: Dict[int, RevHit] = {}\n",
    "    with fitz.open(pdf_path) as d:\n",
    "        n = len(d)\n",
    "    for i in range(n):\n",
    "        res = analyze_page_ocr(pdf_path, i, dpi, use_paddle, use_easy, brx, bry, blocklist, edge_margin)\n",
    "        if not res:\n",
    "            continue\n",
    "        engine, value, score, ctx = res\n",
    "        page_no = i + 1\n",
    "        prev = hits.get(page_no)\n",
    "        if not prev or score > prev.score:\n",
    "            hits[page_no] = RevHit(file=pdf_path.name, page=page_no, value=value,\n",
    "                                   engine=engine, score=score, context_snippet=ctx)\n",
    "    if not hits:\n",
    "        return None\n",
    "    best = max(hits.values(), key=lambda h: getattr(h, 'score', 0))\n",
    "    return best\n",
    "\n",
    "def iter_pdfs(folder: Path) -> Iterable[Path]:\n",
    "    # Avoid duplicates; yield .pdf files only\n",
    "    seen = set()\n",
    "    for p in folder.iterdir():\n",
    "        try:\n",
    "            if p.is_file() and p.suffix.lower() == \".pdf\":\n",
    "                rp = p.resolve()\n",
    "                if rp not in seen:\n",
    "                    seen.add(rp)\n",
    "                    yield p\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# ----------------------------- Pipeline (Native → OCR) -------------------------\n",
    "\n",
    "def run_pipeline(input_folder: Path, output_csv: Path, dpi: int,\n",
    "                 enable_paddle: bool, enable_easy: bool,\n",
    "                 brx: float, bry: float, rev_2l_blocklist: set,\n",
    "                 edge_margin: float) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    pdfs = list(iter_pdfs(input_folder))\n",
    "    if not pdfs:\n",
    "        LOG.warning(f\"No PDFs found in {input_folder}\")\n",
    "\n",
    "    for p in tqdm(pdfs, desc=\"Scanning PDFs\"):\n",
    "        try:\n",
    "            # 1) Native pass (strict ROI, then global)\n",
    "            native_best = process_pdf_native(p, brx, bry, rev_2l_blocklist, edge_margin)\n",
    "\n",
    "            if native_best:\n",
    "                value = _normalize_output_value(native_best.value)\n",
    "                rows.append({\"file\": p.name, \"value\": value, \"engine\": native_best.engine})\n",
    "                continue  # Do NOT OCR this file\n",
    "\n",
    "            # 2) OCR fallback (only if native produced nothing)\n",
    "            ocr_best = process_pdf_ocr(p, dpi, enable_paddle, enable_easy, brx, bry, rev_2l_blocklist, edge_margin)\n",
    "            if ocr_best:\n",
    "                value = _normalize_output_value(ocr_best.value)\n",
    "                rows.append({\"file\": p.name, \"value\": value, \"engine\": ocr_best.engine})\n",
    "                continue\n",
    "\n",
    "            # 3) Neither pipeline produced a value → blank\n",
    "            rows.append({\"file\": p.name, \"value\": \"\", \"engine\": \"\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"Failed {p.name}: {e}\")\n",
    "            rows.append({\"file\": p.name, \"value\": \"\", \"engine\": \"\"})\n",
    "\n",
    "    # --- Write CSV (exactly one row per file, 3 columns) ---\n",
    "    try:\n",
    "        output_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as outf:\n",
    "            writer = csv.writer(outf)\n",
    "            writer.writerow(['file', 'value', 'engine'])\n",
    "            for r in rows:\n",
    "                fs = _scalarize(r.get('file', ''))\n",
    "                vs = _scalarize(r.get('value', ''))\n",
    "                es = _scalarize(r.get('engine', ''))\n",
    "                writer.writerow([fs, vs, es])\n",
    "        LOG.info(f\"Wrote CSV to {output_csv.resolve()} with {len(rows)} rows\")\n",
    "    except Exception as e:\n",
    "        LOG.error(f\"Failed to write CSV: {e}\")\n",
    "\n",
    "    return rows\n",
    "\n",
    "# ----------------------------- CLI --------------------------------------------\n",
    "\n",
    "def parse_args(argv=None):\n",
    "    a = argparse.ArgumentParser(description=\"Extract REV values (bottom-right-first).\")\n",
    "    a.add_argument(\"input_folder\", type=Path)\n",
    "    a.add_argument(\"-o\",\"--output\", type=Path, default=Path(\"rev_results.csv\"))\n",
    "    a.add_argument(\"--dpi\", type=int, default=240, help=\"OCR rasterization DPI\")\n",
    "    a.add_argument(\"--no-paddle\", action=\"store_true\", help=\"Disable PaddleOCR\")\n",
    "    a.add_argument(\"--no-easy\", action=\"store_true\", help=\"Disable EasyOCR\")\n",
    "    a.add_argument(\"--br-x\", type=float, default=DEFAULT_BR_X,\n",
    "                   help=\"Bottom-right ROI X ratio (default 0.68)\")\n",
    "    a.add_argument(\"--br-y\", type=float, default=DEFAULT_BR_Y,\n",
    "                   help=\"Bottom-right ROI Y ratio (default 0.72)\")\n",
    "    a.add_argument(\"--edge-margin\", type=float, default=DEFAULT_EDGE_MARGIN,\n",
    "                   help=\"Fraction of page dims to ignore near edges (default 0.018)\")\n",
    "    a.add_argument(\"--rev-2l-blocklist\", type=str,\n",
    "                   default=\",\".join(sorted(DEFAULT_REV_2L_BLOCKLIST)),\n",
    "                   help=\"Comma-separated two-letter tokens to ignore as REV in ROI (OF is intentionally allowed).\")\n",
    "    return a.parse_args(argv)\n",
    "\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "    blocklist = {s.strip().upper() for s in args.rev_2l_blocklist.split(\",\") if s.strip()}\n",
    "    return run_pipeline(\n",
    "        input_folder=args.input_folder,\n",
    "        output_csv=args.output,\n",
    "        dpi=args.dpi,\n",
    "        enable_paddle=not args.no_paddle,\n",
    "        enable_easy=not args.no_easy,\n",
    "        brx=args.br_x,\n",
    "        bry=args.br_y,\n",
    "        rev_2l_blocklist=blocklist,\n",
    "        edge_margin=args.edge_margin\n",
    "    )\n",
    "\n",
    "def _in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\" and not _in_notebook():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Default Paths for Notebook ----------------------\n",
    "INPUT_FOLDER = Path(r\"INPUT FOLDER PATH\")  # e.g., Path(\"./pdfs\")\n",
    "OUTPUT_CSV   = Path(\"rev_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae75861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native run no OCR\n",
    "args = [str(INPUT_FOLDER), \"-o\", str(OUTPUT_CSV), \"--dpi\", \"240\", \"--no-paddle\", \"--no-easy\"]\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR Run with both OCR engines\n",
    "args = [str(INPUT_FOLDER), \"-o\", str(OUTPUT_CSV), \"--dpi\", \"240\"]\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e157b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25940799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644cdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfextract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
